{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Id\", \"Latitutde\", \"Longitude\", \"target\"]\n",
    "data = pd.read_csv('3D_spatial_network.txt', sep=',', names=columns)\n",
    "data.drop(columns= [\"Id\"], index = range(10000,434874) ,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitutde</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.469079</td>\n",
       "      <td>-1.093255</td>\n",
       "      <td>17.052772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.468515</td>\n",
       "      <td>-1.093958</td>\n",
       "      <td>17.614840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.467917</td>\n",
       "      <td>-1.094435</td>\n",
       "      <td>18.083536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.467490</td>\n",
       "      <td>-1.094649</td>\n",
       "      <td>18.279465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.467078</td>\n",
       "      <td>-1.094643</td>\n",
       "      <td>18.422974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>1.265857</td>\n",
       "      <td>1.503518</td>\n",
       "      <td>45.790861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>1.265069</td>\n",
       "      <td>1.503432</td>\n",
       "      <td>45.475026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>1.264078</td>\n",
       "      <td>1.503180</td>\n",
       "      <td>46.834495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>1.263290</td>\n",
       "      <td>1.503108</td>\n",
       "      <td>48.185708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>1.262207</td>\n",
       "      <td>1.503035</td>\n",
       "      <td>48.704579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Latitutde  Longitude     target\n",
       "0     -0.469079  -1.093255  17.052772\n",
       "1     -0.468515  -1.093958  17.614840\n",
       "2     -0.467917  -1.094435  18.083536\n",
       "3     -0.467490  -1.094649  18.279465\n",
       "4     -0.467078  -1.094643  18.422974\n",
       "...         ...        ...        ...\n",
       "9995   1.265857   1.503518  45.790861\n",
       "9996   1.265069   1.503432  45.475026\n",
       "9997   1.264078   1.503180  46.834495\n",
       "9998   1.263290   1.503108  48.185708\n",
       "9999   1.262207   1.503035  48.704579\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.asarray(data.iloc[:-2000, :-1])\n",
    "y_train = np.asarray(data.target[:-2000])\n",
    "test_X = np.asarray(data.iloc[-2000:,:-1])\n",
    "y_test = np.asarray(data.target[-2000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalise\n",
    "\n",
    "for i in range(train_X.shape[1]):\n",
    "    train_X[:, i] = (train_X[:, i] - train_X[:, i].mean())/ train_X[:,i].std()\n",
    "    test_X[:, i] = (test_X[:, i] - test_X[:, i].mean())/ test_X[:,i].std()\n",
    "\n",
    "y_train = (y_train - y_train.mean())/ y_train.std()\n",
    "y_test = (y_test - y_test.mean()) / y_test.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_train_dataset(train, degree):\n",
    "    if degree == 1:\n",
    "        return train\n",
    "    \n",
    "    train_prev = np.copy(train)\n",
    "    out = np.copy(train_prev)\n",
    "    \n",
    "    for i in range(2, degree + 1):\n",
    "        train_prev = multiply_dataset(train_prev, train, i)\n",
    "        out = np.concatenate((out, train_prev), axis = 1)\n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_dataset(train_prev, train, degree):\n",
    "#     print(train_prev)\n",
    "#     print(train)\n",
    "    train_degree = np.copy(train_prev)\n",
    "    for i in range(train.shape[1]):\n",
    "        train_degree[:,i] = train_degree[:, i] * train[:, 0]\n",
    "    train_degree = np.insert(train_degree, train_degree.shape[1], train[:,-1] ** degree, axis=1)\n",
    "    return train_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 3\n",
    "train_X_degree = modify_train_dataset(train_X, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Method\n",
      "intercept :  -1.343543332144037e-15\n",
      "weights :  [ 0.16006403 -0.09090649]\n",
      "\n",
      "loss on test set :  1818.2618605279426\n",
      "RMSE loss:  0.953483576294826\n",
      "\n",
      "loss on train set :  7814.131056322699\n",
      "RMSE loss:  0.9883149204784563\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrix Method\")\n",
    "X_train = np.insert(train_X, 0, 1, axis=1)\n",
    "X_test = np.insert(test_X, 0, 1, axis=1)\n",
    "\n",
    "XTX_inv = np.linalg.inv(np.matmul(X_train.T, X_train))\n",
    "moore_penrose_pseudo_inv = np.matmul(XTX_inv, X_train.T)\n",
    "weights = np.matmul(moore_penrose_pseudo_inv, y_train)\n",
    "\n",
    "print('intercept : ', weights[0])\n",
    "print('weights : ', weights[1:])\n",
    "\n",
    "y_pred = np.matmul(X_test, weights)\n",
    "error = y_pred - y_test\n",
    "loss_min = np.matmul(error.T, error)\n",
    "print('\\nloss on test set : ', loss_min)\n",
    "print('RMSE loss: ', np.sqrt(loss_min/X_test.shape[0]))\n",
    "\n",
    "\n",
    "y_pred = np.matmul(X_train, weights)\n",
    "error = y_pred - y_train\n",
    "loss_min = np.matmul(error.T, error)\n",
    "print('\\nloss on train set : ', loss_min)\n",
    "print('RMSE loss: ', np.sqrt(loss_min/X_train.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_ = np.random.random(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W_ = np.copy(weights)\n",
    "# W_ = [ 0.001, 0.200, -0.20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before gradient descent, Random initialisation of W\n",
      "\n",
      "loss on test set :  3700.112829282558\n",
      "RMSE loss:  1.3601677891500294\n",
      "\n",
      "loss on train set :  15703.090152041936\n",
      "RMSE loss:  1.401030431148889\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.matmul(X_test, W_)\n",
    "error = y_pred - y_test\n",
    "loss_min = np.matmul(error.T, error)\n",
    "print(\"Before gradient descent, Random initialisation of W\")\n",
    "print('\\nloss on test set : ', loss_min)\n",
    "print('RMSE loss: ', np.sqrt(loss_min/X_test.shape[0]))\n",
    "\n",
    "y_pred = np.matmul(X_train, W_)\n",
    "error = y_pred - y_train\n",
    "loss_min = np.matmul(error.T, error)\n",
    "print('\\nloss on train set : ', loss_min)\n",
    "print('RMSE loss: ', np.sqrt(loss_min/X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.39348349  0.1419395  -0.04669549]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-9\n",
    "X = np.insert(train_X,0,1,axis=1)\n",
    "\n",
    "W1 = np.copy(W_)\n",
    "# print(W)\n",
    "loss = []\n",
    "\n",
    "run=0\n",
    "\n",
    "while True:\n",
    "    run += 1\n",
    "    y_pred = np.matmul(X, W1)\n",
    "    error = y_pred - y_train\n",
    "    loss_val = 1/2*np.matmul(error.T, error)\n",
    "    loss.append(loss_val)\n",
    "    \n",
    "    grad = np.matmul(error.T, X)\n",
    "    \n",
    "    W1 = W1 - learning_rate*grad\n",
    "    if(run==1):\n",
    "        continue\n",
    "    if(loss[run-2]-loss[run-1]< 0.01):\n",
    "        break\n",
    "    \n",
    "    print(run, end = \"\\r\")\n",
    "print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115097"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Gradient descent\n",
      "\n",
      "loss on test set :  2112.865089930333\n",
      "RMSE loss:  1.0278290446203426\n",
      "\n",
      "loss on train set :  9066.33776021314\n",
      "RMSE loss:  1.0645619850561274\n",
      "Intercept:  0.3934834947553149\n",
      "[ 0.1419395  -0.04669549]\n"
     ]
    }
   ],
   "source": [
    "print(\"From Gradient descent\")\n",
    "\n",
    "y_pred = np.matmul(X_test, W1)\n",
    "error = y_pred - y_test\n",
    "loss_min = np.matmul(error.T, error)\n",
    "print('\\nloss on test set : ', loss_min)\n",
    "print('RMSE loss: ', np.sqrt(loss_min/X_test.shape[0]))\n",
    "\n",
    "y_pred = np.matmul(X_train, W1)\n",
    "error = y_pred - y_train\n",
    "loss_min = np.matmul(error.T, error)\n",
    "print('\\nloss on train set : ', loss_min)\n",
    "print('RMSE loss: ', np.sqrt(loss_min/X_train.shape[0]))\n",
    "\n",
    "print(\"Intercept: \", W1[0])\n",
    "print(W1[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01823482  0.12028969 -0.1068097 ]\n"
     ]
    }
   ],
   "source": [
    "### stochastic\n",
    "W = np.copy(W_)\n",
    "loss = []\n",
    "run = 0\n",
    "learning_rate = 1e-4\n",
    "while True:\n",
    "    run += 1\n",
    "    error = 0\n",
    "#     print(run, end = \"\\r\")\n",
    "    for i in range(X_train.shape[0]):\n",
    "        y_pred = np.dot(X_train[i], W)\n",
    "        loss_val = y_pred - y_train[i]\n",
    "        grad = loss_val * X_train[i]\n",
    "        error += loss_val**2\n",
    "        W = W - learning_rate * grad\n",
    "    \n",
    "    loss.append(error)\n",
    "    \n",
    "    if run == 1:\n",
    "        continue\n",
    "    \n",
    "    if(loss[run-2] - loss[run-1] < 0.01):\n",
    "        break\n",
    "    print(error, end = \"\\r\")\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From stochastic Gradient Descent\n",
      "\n",
      "loss on test set :  951.0400582773357\n",
      "RMSE loss:  0.6895796031921679\n",
      "\n",
      "loss on train set :  3917.5872144074524\n",
      "RMSE loss:  0.6997845395555203\n"
     ]
    }
   ],
   "source": [
    "print(\"From stochastic Gradient Descent\")\n",
    "\n",
    "y_pred = np.matmul(X_test, W)\n",
    "error = y_pred - y_test\n",
    "loss_min = 1/2*np.matmul(error.T, error)\n",
    "print('\\nloss on test set : ', loss_min)\n",
    "print('RMSE loss: ', np.sqrt(loss_min/X_test.shape[0]))\n",
    "\n",
    "y_pred = np.matmul(X_train, W)\n",
    "error = y_pred - y_train\n",
    "loss_min = 1/2*np.matmul(error.T, error)\n",
    "print('\\nloss on train set : ', loss_min)\n",
    "print('RMSE loss: ', np.sqrt(loss_min/X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_no_reg = np.matmul(X_train, W)\n",
    "error = y_pred - train_y\n",
    "loss_min = 1/2*np.matmul(error.T, error)\n",
    "print('\\nloss on train set : ', loss_min)\n",
    "print('RMSE loss: ', np.sqrt(loss_min/8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = np.insert(train_X,0,1,axis=1)\n",
    "\n",
    "train_X_reg = X[:-1500]\n",
    "train_y_reg = train_y[:-1500]\n",
    "\n",
    "val_X = X[-1500:]\n",
    "val_y = train_y[-1500:]\n",
    "\n",
    "learning_rate = 1e-9\n",
    "lmd_2 = [1]\n",
    "\n",
    "W = W_\n",
    "print(W)\n",
    "\n",
    "loss_valid = []\n",
    "for i,lmd in enumerate(lmd_2):\n",
    "    W = W_\n",
    "    loss = [] \n",
    "    run = 0\n",
    "    while True:\n",
    "        run += 1\n",
    "        pred_y = np.matmul(X,W)\n",
    "        error = pred_y - train_y\n",
    "        loss_val = 1/2*np.matmul(error.T,error) + lmd/2*np.matmul(W.T,W)\n",
    "        loss.append(loss_val)\n",
    "        grad = np.matmul(error.T,X) + lmd*W\n",
    "        W = W - learning_rate*grad\n",
    "        if(run==1):\n",
    "            continue\n",
    "        # previous loss (run - 1 - 1) minus current loss (run - 1)\n",
    "        if(loss[run-2]-loss[run-1]<0.01):\n",
    "            break\n",
    "    \n",
    "    print('\\nintercept : ',W[0])\n",
    "    print('weights : ',W[1:])\n",
    "    print(\"lamda : {} , runs : {} , loss on training : {}\".format(lmd,run,loss_val))\n",
    "\n",
    "    pred_y = np.matmul(val_X,W)\n",
    "    error = pred_y - val_y\n",
    "    loss_valid.append(1/2*np.matmul(error.T,error))\n",
    "    print('loss on val set : ',loss_valid[i])\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~\")\n",
    "    \n",
    "\n",
    "plt.plot(lmd_2,loss_valid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.insert(train_X,0,1,axis=1)\n",
    "\n",
    "train_X_reg = X[:-1500]\n",
    "train_y_reg = train_y[:-1500]\n",
    "\n",
    "val_X = X[-1500:]\n",
    "val_y = train_y[-1500:]\n",
    "\n",
    "learning_rate = 1e-10\n",
    "lmd_1 = [1e-5,1e-4,1e-3,1e-2,1e-1,1,10,100,1000,10000,100000]\n",
    "\n",
    "W = W_\n",
    "print(W)\n",
    "\n",
    "loss_valid = []\n",
    "\n",
    "for i,lmd in enumerate(lmd_1):\n",
    "    W = W_\n",
    "    loss = [] \n",
    "    run = 0\n",
    "    while True:\n",
    "        run += 1\n",
    "        pred_y = np.matmul(X,W)\n",
    "        error = pred_y - train_y\n",
    "        loss_val = 1/2*np.matmul(error.T,error) + lmd/2*np.sum(abs(W))\n",
    "        loss.append(loss_val)\n",
    "        grad = np.matmul(error.T,X) + lmd/2*np.sign(W)\n",
    "        W = W - learning_rate*grad\n",
    "        if(run==1):\n",
    "            continue\n",
    "        \n",
    "        if(loss[run-2]-loss[run-1] < 0.01): \n",
    "            break\n",
    "    \n",
    "    print('\\nintercept : ',W[0])\n",
    "    print('weights : ',W[1:])\n",
    "    print(\"lamda : {} , runs : {} , loss on training : {}\".format(lmd,run,loss_val))\n",
    "\n",
    "    pred_y = np.matmul(val_X,W)\n",
    "    error = pred_y - val_y\n",
    "    loss_valid.append(1/2*np.matmul(error.T,error))\n",
    "    print('loss on val set : ',loss_valid[i])\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
